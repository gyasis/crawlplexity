{
  "model": {
    "type": "string",
    "default": null,
    "description": "The model to use for completion",
    "required": true,
    "essential": true,
    "default_active": true,
    "ui_type": "select",
    "category": "core",
    "tooltip": "üéØ ESSENTIAL: Specifies which AI model to use (e.g., 'gpt-4o', 'claude-3-sonnet', 'ollama/mistral-nemo'). This parameter is absolutely required for all API calls.",
    "collapsible_section": "core",
    "can_deactivate": false
  },
  "messages": {
    "type": "array",
    "default": [],
    "description": "List of messages for the conversation",
    "required": true,
    "essential": true,
    "default_active": true,
    "ui_type": "hidden",
    "category": "core",
    "tooltip": "üí¨ ESSENTIAL: The conversation history as an array of message objects with 'role' and 'content' fields. Required for all chat completions.",
    "collapsible_section": "core",
    "can_deactivate": false
  },
  "max_tokens": {
    "type": "integer",
    "default": 16000,
    "minimum": 1,
    "maximum": 200000,
    "description": "The maximum number of tokens to generate in the completion",
    "required": false,
    "essential": true,
    "default_active": true,
    "alert_threshold": 190000,
    "ui_type": "number",
    "category": "generation",
    "tooltip": "üö® CRITICAL: Maximum tokens to generate. ESSENTIAL for Anthropic (Claude) - API will fail without it! Default: 16K tokens, Max: 200K. Alert at 190K+.",
    "collapsible_section": "generation",
    "can_deactivate": false
  },
  "temperature": {
    "type": "number",
    "default": 0.7,
    "minimum": 0.0,
    "maximum": 1.0,
    "step": 0.1,
    "description": "Controls randomness. Higher values (0.8) make output more random, lower values (0.2) more focused and deterministic",
    "required": false,
    "essential": false,
    "default_active": true,
    "ui_type": "range",
    "category": "generation",
    "tooltip": "üå°Ô∏è Controls creativity vs focus. 0.0 = focused/deterministic, 0.7 = balanced, 1.0 = creative/random. Supported by all providers.",
    "collapsible_section": "generation",
    "can_deactivate": true
  },
  "top_p": {
    "type": "number",
    "default": 1.0,
    "minimum": 0.0,
    "maximum": 1.0,
    "step": 0.01,
    "description": "Nucleus sampling: considers tokens with top_p probability mass. 0.1 means only top 10% probability tokens",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "range",
    "category": "generation",
    "tooltip": "üéØ Nucleus sampling - limits token selection to top % probability mass. 0.1 = only top 10% likely tokens, 1.0 = all tokens. Alternative to temperature.",
    "collapsible_section": "generation",
    "can_deactivate": true
  },
  "n": {
    "type": "integer",
    "default": 1,
    "minimum": 1,
    "maximum": 10,
    "description": "How many chat completion choices to generate for each input message",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "number",
    "category": "generation",
    "tooltip": "üî¢ Number of different responses to generate. Costs multiply by n (2 choices = 2x cost). Mainly useful for A/B testing responses.",
    "collapsible_section": "generation",
    "can_deactivate": true
  },
  "stream": {
    "type": "boolean",
    "default": false,
    "description": "Whether to stream back partial progress",
    "required": false,
    "essential": false,
    "default_active": true,
    "ui_type": "checkbox",
    "category": "output",
    "tooltip": "üåä STREAMING: Enable real-time response streaming. Supported by all providers. Essential for chat UX - shows words as they're generated.",
    "collapsible_section": "output",
    "can_deactivate": true
  },
  "presence_penalty": {
    "type": "number",
    "default": 0.0,  
    "minimum": -2.0,
    "maximum": 2.0,
    "step": 0.1,
    "description": "Penalizes new tokens based on whether they appear in the text so far. Positive values increase likelihood of new topics",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "range",
    "category": "penalties",
    "tooltip": "üö´ Reduces repetition of topics/concepts. 0.0 = no penalty, +1.0 = encourage new topics, -1.0 = allow repetition. OpenAI/compatible models only.",
    "collapsible_section": "penalties",
    "can_deactivate": true
  },
  "frequency_penalty": {
    "type": "number",
    "default": 0.0,
    "minimum": -2.0,
    "maximum": 2.0,
    "step": 0.1,
    "description": "Penalizes new tokens based on their existing frequency. Positive values decrease likelihood of repetition",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "range",
    "category": "penalties",
    "tooltip": "üîÅ Reduces repetition of exact words/phrases. 0.0 = no penalty, +1.0 = strongly avoid repetition. Works with presence_penalty for comprehensive anti-repetition.",
    "collapsible_section": "penalties",
    "can_deactivate": true
  },
  "logit_bias": {
    "type": "object",
    "default": {},
    "description": "Modify likelihood of specified tokens. Maps token IDs to bias values from -100 to 100",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "json",
    "category": "advanced",
    "tooltip": "üéõÔ∏è ADVANCED: Fine-tune specific token probabilities. Maps token IDs to bias (-100 to +100). Requires tokenizer knowledge - use with caution!",
    "collapsible_section": "advanced",
    "can_deactivate": true,
    "modal_only": true
  },
  "stop": {
    "type": "array",
    "default": null,
    "description": "Up to 4 sequences where the API will stop generating further tokens",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "textarea",
    "category": "generation",
    "tooltip": "‚èπÔ∏è Stop sequences - generation halts when these strings are encountered. Max 4 sequences. Example: ['\\n\\n', 'END', '---']",
    "collapsible_section": "generation",
    "can_deactivate": true
  },
  "user": {
    "type": "string",
    "default": null,
    "description": "A unique identifier representing your end-user, which can help with monitoring and abuse detection",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "text",
    "category": "metadata",
    "tooltip": "üë§ User identifier for tracking/monitoring. Helps with abuse detection and usage analytics. Not required but recommended for production.",
    "collapsible_section": "metadata",
    "can_deactivate": true
  },
  "top_k": {
    "type": "integer",
    "default": -1,
    "minimum": -1,
    "maximum": 100,
    "description": "Limits the number of highest probability vocabulary tokens to keep for top-k-filtering",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "number",
    "category": "generation",
    "tooltip": "üîù Top-K sampling - only consider K most likely tokens. -1 = disabled, 40 = consider top 40 tokens. Alternative to top_p. Ollama/local models mainly.",
    "collapsible_section": "generation",
    "can_deactivate": true
  },
  "repetition_penalty": {
    "type": "number",
    "default": 1.0,
    "minimum": 0.1,
    "maximum": 2.0,
    "step": 0.1,
    "description": "The penalty for repeating tokens. 1.0 means no penalty",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "range",
    "category": "penalties",
    "tooltip": "üîÑ Repetition penalty - discourages repeating tokens. 1.0 = no penalty, 1.1 = slight penalty, 1.5+ = strong penalty. Ollama/local models mainly.",
    "collapsible_section": "penalties",
    "can_deactivate": true
  },
  "min_p": {
    "type": "number",
    "default": 0.0,
    "minimum": 0.0,
    "maximum": 1.0,
    "step": 0.01,
    "description": "Minimum probability for a token to be considered. Alternative to top_p",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "range",
    "category": "generation",
    "tooltip": "üìä Min-P sampling - sets minimum probability threshold. 0.0 = all tokens allowed, 0.05 = only tokens with 5%+ probability. Advanced alternative to top_p.",
    "collapsible_section": "generation",
    "can_deactivate": true,
    "modal_only": true
  },
  "typical_p": {
    "type": "number",
    "default": 1.0,
    "minimum": 0.0,
    "maximum": 1.0,
    "step": 0.01,
    "description": "Typical sampling parameter. Controls the cumulative probability of typical tokens",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "range",
    "category": "generation",
    "tooltip": "üìà Typical sampling - focuses on 'typical' tokens with average surprisal. 1.0 = disabled, 0.9 = focus on typical tokens. Advanced sampling method.",
    "collapsible_section": "generation",
    "can_deactivate": true,
    "modal_only": true
  },
  "seed": {
    "type": "integer",
    "default": null,
    "minimum": 0,
    "maximum": 2147483647,
    "description": "Random seed for deterministic outputs. Same seed should produce same results",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "number",
    "category": "advanced",
    "tooltip": "üé≤ Deterministic seed - same seed produces same output (mostly). Useful for testing/reproducibility. Works with OpenAI, some others.",
    "collapsible_section": "advanced",
    "can_deactivate": true
  },
  "response_format": {
    "type": "object",
    "default": null,
    "description": "Format for the response (e.g., JSON mode for some models)",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "json",
    "category": "output",
    "tooltip": "üìù JSON mode - forces structured output. Use {\"type\": \"json_object\"} for JSON responses. OpenAI GPT-4/3.5, some others support this.",
    "collapsible_section": "output",
    "can_deactivate": true
  },
  "timeout": {
    "type": "number",
    "default": 600,
    "minimum": 1,
    "maximum": 3600,
    "description": "Request timeout in seconds",
    "required": false,
    "essential": false,
    "default_active": true,
    "ui_type": "number",
    "category": "system",
    "tooltip": "‚è±Ô∏è Request timeout - how long to wait for response. 600s default, 60s for quick tasks, 1800s+ for long generations.",
    "collapsible_section": "system",
    "can_deactivate": true
  },
  "logprobs": {
    "type": "boolean",
    "default": false,
    "description": "Whether to return log probabilities of the output tokens",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "checkbox",
    "category": "output",
    "tooltip": "üìä Log probabilities - returns confidence scores for each token. Useful for analysis/debugging. OpenAI models mainly.",
    "collapsible_section": "output",
    "can_deactivate": true,
    "modal_only": true
  },
  "top_logprobs": {
    "type": "integer",
    "default": null,
    "minimum": 0,
    "maximum": 20,
    "description": "Number of most likely tokens to return at each position (requires logprobs=true)",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "number",
    "category": "output",
    "tooltip": "üîù Top log probabilities - shows N most likely alternative tokens at each position. Requires logprobs=true. Advanced debugging tool.",
    "collapsible_section": "output",
    "can_deactivate": true,
    "modal_only": true
  },
  "functions": {
    "type": "array",
    "default": null,
    "description": "DEPRECATED: List of functions the model may generate JSON inputs for. Use tools instead",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "json",
    "category": "deprecated",
    "tooltip": "‚ö†Ô∏è DEPRECATED: Old function calling format. Use 'tools' instead. Only enable if using legacy OpenAI function calling.",
    "collapsible_section": "deprecated",
    "can_deactivate": true,
    "modal_only": true
  },
  "function_call": {
    "type": "object",
    "default": null,
    "description": "DEPRECATED: Controls which function the model calls. Use tool_choice instead",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "json",
    "category": "deprecated",
    "tooltip": "‚ö†Ô∏è DEPRECATED: Old function choice format. Use 'tool_choice' instead. Only for legacy compatibility.",
    "collapsible_section": "deprecated",
    "can_deactivate": true,
    "modal_only": true
  },
  "tools": {
    "type": "array",
    "default": null,
    "description": "List of tools the model may call. Currently only 'function' is supported",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "json",
    "category": "tools",
    "tooltip": "üîß Function calling - define tools/functions the model can call. Array of function definitions. OpenAI, Anthropic, Google support this.",
    "collapsible_section": "tools",
    "can_deactivate": true
  },
  "tool_choice": {
    "type": "object",
    "default": null,
    "description": "Controls which tool is called by the model",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "json",
    "category": "tools",
    "tooltip": "üéØ Tool choice - control which function to call. 'auto' = model decides, 'none' = no tools, {\"type\": \"function\", \"function\": {\"name\": \"func_name\"}}.",
    "collapsible_section": "tools",
    "can_deactivate": true
  },
  "api_base": {
    "type": "string",
    "default": null,
    "description": "LiteLLM: Override API endpoint URL",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "text",
    "category": "system",
    "tooltip": "üåê Custom API endpoint - override default provider URLs. Example: 'https://api.custom-provider.com/v1'. For custom/self-hosted models.",
    "collapsible_section": "system",
    "can_deactivate": true
  },
  "api_version": {
    "type": "string",
    "default": null,
    "description": "LiteLLM: API version (Azure-specific)",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "text",
    "category": "system",
    "tooltip": "üîÑ API version - mainly for Azure OpenAI. Example: '2023-12-01-preview'. Auto-detected for most providers.",
    "collapsible_section": "system",
    "can_deactivate": true,
    "modal_only": true
  },
  "num_retries": {
    "type": "integer",
    "default": 0,
    "minimum": 0,
    "maximum": 10,
    "description": "LiteLLM: Number of retries for failed API calls",
    "required": false,
    "essential": false,
    "default_active": true,
    "ui_type": "number",
    "category": "system",
    "tooltip": "üîÑ Auto-retry failed requests. 0 = no retries, 3 = retry up to 3 times. Handles rate limits and transient errors automatically.",
    "collapsible_section": "system",
    "can_deactivate": true
  },
  "fallbacks": {
    "type": "array",
    "default": null,
    "description": "LiteLLM: List of fallback models if the primary fails",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "json",
    "category": "system",
    "tooltip": "üö™ Fallback models - automatically try alternatives if primary model fails. Array of model names: ['gpt-3.5-turbo', 'claude-3-haiku'].",
    "collapsible_section": "system",
    "can_deactivate": true,
    "modal_only": true
  },
  "drop_params": {
    "type": "boolean",
    "default": false,
    "description": "LiteLLM: Drop unsupported parameters instead of raising error",
    "required": false,
    "essential": false,
    "default_active": true,
    "ui_type": "checkbox",
    "category": "system",
    "tooltip": "üóëÔ∏è Drop unsupported params - silently ignore parameters not supported by the model instead of erroring. Recommended: true.",
    "collapsible_section": "system",
    "can_deactivate": true
  },
  "task_type": {
    "type": "string",
    "default": "general",
    "enum": ["general", "search", "summary", "followup", "coding", "creative", "research"],
    "description": "Custom task type for model selection optimization",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "select",
    "category": "system",
    "tooltip": "üéØ Task type hint - helps optimize model selection for specific use cases. 'coding' for code, 'creative' for writing, etc. Custom Fireplexity feature.",
    "collapsible_section": "system",
    "can_deactivate": true
  },
  "strategy": {
    "type": "string", 
    "default": "balanced",
    "enum": ["performance", "cost", "balanced", "local"],
    "description": "Model selection strategy when no specific model is chosen",
    "required": false,
    "essential": false,
    "default_active": false,
    "ui_type": "select",
    "category": "system",
    "tooltip": "‚öôÔ∏è Selection strategy - 'performance' prioritizes best models, 'cost' cheapest, 'balanced' compromise, 'local' prefers Ollama. Custom Fireplexity feature.",
    "collapsible_section": "system",
    "can_deactivate": true
  },
  "meta": {
    "version": "2.0.0",
    "last_updated": "2025-01-25",
    "total_parameters": 32,
    "collapsible_sections": {
      "core": {
        "title": "Core Parameters",
        "description": "Essential parameters required for all API calls",
        "icon": "üéØ",
        "default_expanded": true,
        "priority": 1
      },
      "generation": {
        "title": "Generation Control",
        "description": "Parameters controlling text generation behavior",
        "icon": "‚ö°",
        "default_expanded": true,
        "priority": 2
      },
      "output": {
        "title": "Output & Streaming",
        "description": "Control response format and streaming behavior",
        "icon": "üì§",
        "default_expanded": true,
        "priority": 3
      },
      "penalties": {
        "title": "Repetition Control",
        "description": "Parameters to reduce repetition and control penalties",
        "icon": "üö´",
        "default_expanded": false,
        "priority": 4
      },
      "tools": {
        "title": "Function Calling",
        "description": "Function/tool calling configuration",
        "icon": "üîß",
        "default_expanded": false,
        "priority": 5
      },
      "system": {
        "title": "System & LiteLLM",
        "description": "System configuration and LiteLLM-specific parameters",
        "icon": "‚öôÔ∏è",
        "default_expanded": false,
        "priority": 6
      },
      "advanced": {
        "title": "Advanced Options",
        "description": "Advanced parameters for fine-tuning model behavior",
        "icon": "üéõÔ∏è",
        "default_expanded": false,
        "priority": 7
      },
      "metadata": {
        "title": "Metadata & Tracking",
        "description": "User identification and tracking parameters",
        "icon": "üë§",
        "default_expanded": false,
        "priority": 8
      },
      "deprecated": {
        "title": "Deprecated",
        "description": "Legacy parameters - use alternatives instead",
        "icon": "‚ö†Ô∏è",
        "default_expanded": false,
        "priority": 9
      }
    },
    "parameter_counts": {
      "total": 32,
      "essential": 3,
      "default_active": 6,
      "modal_only": 8,
      "can_deactivate": 29
    },
    "categories": {
      "core": "Essential parameters for model selection",
      "generation": "Text generation control parameters", 
      "penalties": "Parameters to control repetition and penalties",
      "advanced": "Advanced model behavior parameters",
      "output": "Output format and streaming parameters",
      "system": "System and LiteLLM configuration parameters",
      "tools": "Function calling and tool usage parameters",
      "metadata": "User identification and tracking parameters",
      "deprecated": "Legacy parameters - avoid using"
    }
  }
}